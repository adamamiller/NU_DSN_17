{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scikit-learn Soup to Nuts: <br> Developing a Machine-Learning Workflow\n",
    "\n",
    "In this lecture we will discuss the tools and steps necessary to build a successful machine-learning model.\n",
    "\n",
    "<br>\n",
    "<center> Adam A Miller  \n",
    "CIERA/Northwestern & Adler Planetarium  \n",
    "(c) 2017 Nov 2</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; fundamentally concerned with the problem of classification   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *particularly in regime of large dimensional data sets*  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (methods can be extended to regression)  \n",
    "  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; (glorified) Pattern Matching  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *a slight over-simplification* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In other words, be careful about over-interpreting the \"learning\"..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/DL_vs_humans.png\" width=850cm>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<font size=\"1\"> *credit*: this image is everywhere on the web, and I cannot track down original source, [this blog](https://devblogs.nvidia.com/parallelforall/mocha-jl-deep-learning-julia/) uses it without attribution </font>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Terminology\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **Features**   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; measured properties of objects in the data set  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; can be numerical or categorical (e.g., red vs. blue)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **Labels**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; target classification or regression variable (to be predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Standard ([supervised](https://en.wikipedia.org/wiki/Supervised_learning)) ML goal: \n",
    "  1. **Train** — Develop a mapping between *features* and *labels*\n",
    "\n",
    "  2. **Test** — Evaluate model on non-training labeled data\n",
    "\n",
    "  3. **Predict** — Apply model to sources with unknown labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Today I will not discuss [unsupervised machine learning](https://en.wikipedia.org/wiki/Unsupervised_learning). Primarily because we do not have time, but also because I have not seen a single useful application of these techniques in my own science.\n",
    "\n",
    "In brief, unsupervised learning ignores any labels that may be available and instead attempts to cluster sources based on their similarity in the multidimensional feature space. However, once the clusters have been identified there is no mathematical method for measuring the quality of the clusters (and hence my feelings that these methods aren't that useful)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/ML_summary.png\" width=850cm>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Why is the step with the test data necessary?\n",
    "\n",
    "[Take a few min to dicuss with your partner]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With this simple picture in mind, let's get started.\n",
    "\n",
    "Our tool for today is `python`'s [scikit-learn](http://scikit-learn.org/stable/).\n",
    "\n",
    "`scikit-learn` is amazing! It includes everything needed to construct the ML workflow, and has excellent documentation. \n",
    "\n",
    "With only 4 (!) lines of code, we can build a ML model with `scitkit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "iris = load_iris()\n",
    "rf_clf = RandomForestClassifier().fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bang\n",
    "\n",
    "Just like that - you're done. \n",
    "\n",
    "Now you can all go home."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a very important aside - allow me a moment on my soapbox to advise caution regarding the simplicity of `scikit-learn`: the package is so user friendly, and documentation so good, that it is not just easy to build a model, but it is also incredibly easy to become over confident in the model. Generally speaking, ML models are highly subject to noise and training-set biases and the simplicity of `scikit-learn` can consistently lead to a few lines of code that appear to produce outstanding results.\n",
    "\n",
    "This is the first (but it will not be the last) time that I will implore you to **worry about the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On to building a full pipeline..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "As ML is a data-driven method, the first, and arguably most important step is to curate the data.\n",
    "\n",
    "  1. Query, observe, simulate, etc.   \n",
    "&nbsp;(i.e. collect the observations)\n",
    "  2. Select features to be used in the model\n",
    "  3. Determine training set \"ground truth\" (i.e. *labels*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Beyond these initial considerations, additional setps to consider include:  \n",
    "\n",
    "&nbsp;&nbsp; 4. Convert categorical features  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; e.g., male, female, male, male $\\rightarrow$ [0, 1, 0, 0]  \n",
    "&nbsp;&nbsp; 5. [Impute](https://en.wikipedia.org/wiki/Imputation_(statistics) (or discard) missing data  \n",
    "&nbsp;&nbsp; 6. Feature normalization  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; typically only necessary for certain ML models  \n",
    "&nbsp;&nbsp; 7. Visualize the data   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a critical step for all data-science applications  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "and of course, don't forget...\n",
    "\n",
    "# Worry About the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Today we will work with the famous [iris flower data set](https://en.wikipedia.org/wiki/Iris_flower_data_set), which is small and understandable, but as a result avoids many of the trappings of dealing with real world data. \n",
    "\n",
    "There are 3 iris classes: setosa, virginica, and versicolor.\n",
    "\n",
    "For each flower, 4 features have been measured: petal length, petal width, sepal length, and sepal width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will use [`seaborn`](https://seaborn.pydata.org) to visualize the data (but all subsequent work will be in `scikit-learn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "xkcd_colors = [\"windows blue\", \"amber\", \"slate\"]\n",
    "\n",
    "def infer_cmap(color):\n",
    "    xkcd_colors = [\"windows blue\", \"amber\", \"slate\"]\n",
    "    hues = sns.xkcd_palette(xkcd_colors)\n",
    "    if color == hues[0]:\n",
    "        return sns.light_palette(hues[0], as_cmap=True)\n",
    "    elif color == hues[1]:\n",
    "        return sns.light_palette(hues[1], as_cmap=True)\n",
    "    elif color == hues[2]:\n",
    "        return sns.light_palette(hues[2], as_cmap=True)\n",
    "\n",
    "def kde_color_plot(x, y, **kwargs):\n",
    "    cmap = infer_cmap(kwargs['color'])\n",
    "    ax = sns.kdeplot(x, y, shade=True, shade_lowest=False, cmap=cmap, **kwargs)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "iris_df = sns.load_dataset(\"iris\")\n",
    "g = sns.PairGrid(iris_df, hue='species', \n",
    "                 vars=['sepal_length','sepal_width',\n",
    "                       'petal_length','petal_width'], \n",
    "                 palette=sns.xkcd_palette(xkcd_colors), \n",
    "                 diag_sharey=False)\n",
    "g = g.map_upper(plt.scatter, alpha=0.7)\n",
    "g = g.map_lower(kde_color_plot)\n",
    "g = g.map_diag(sns.kdeplot, shade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In brief, these corner plots show that the data are fairly well separated, though there is some overlap between the virginica and versicolor species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "[This step may need to be repeated]\n",
    "\n",
    "Add new features (if necessary)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Utilize domain knowledge to create/compute new features    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; e.g., sepal_length/petal_length may be more informative\n",
    "\n",
    "Remove noisy/uniformative features (if necessary)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; [Feature importance can be measured](http://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation) via Random Forest  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; [Forward/backward feature selection](https://www.cs.cmu.edu/~kdeng/thesis/feature.pdf) can thin feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this case we have only 4 features, so we will proceed under the assumption that the feature set need not be thinned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Model Selection\n",
    "\n",
    "[This step may need to be repeated]\n",
    "\n",
    "Following data organization and feature engineering, the practitioner must then select an ML algorithm.\n",
    "\n",
    "Every problem/data set is different. Best practices often include trying multiple algorithms to determine which is best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After lots of experience it is possible to develop some intuition for which algorithms will work best in which regimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But remember - ultimately we are working with black boxes. Intuition can easily lead you astray in this case..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will adopt the [$k$-Nearest Neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) ($k$NN) algorithm for today's problem. The primary reason is that it is very easy to understand:\n",
    "\n",
    "Final classifications are determined by identifying the $k$, a user-selected number, nearest neighbors in the training set to the source being classified. Euclidean distances are typically used to determine the separation between sources, though other metrics are also possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$k$NN is an algorithm that may require feature normalization (discussed above). \n",
    "\n",
    "Imagine, for example, a two feature model where feature $x_1$ is gaussian distributed with mean 0 and standard deviation 10 $[x_1 \\sim \\mathcal{N}(0, 100)]$, compared to feature $x_2 \\sim \\mathcal{N}(0,0.01)$. In this case, the classifications will be entirely decided by $x_1$ as the typical $\\Delta x_1$ will be orders of magnitude larger than $\\Delta x_2$. \n",
    "\n",
    "Of course, if $x_1$ is significantly more important than $x_2$ then maybe this is okay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`scikit-learn` makes $k$NN easy with the [`KNeighborsClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) class from the [`neighbors`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors) subpackage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You may have noticed that I set $k = 11$. This should worry you - why 11 neighbors and not 7? or the default 5? or 121?\n",
    "\n",
    "We will answer that now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The real answer for why I set $k = 11$ is that today I got a [slurpie](https://en.wikipedia.org/wiki/Slurpee) and it tasted good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Model Evaluation\n",
    "\n",
    "[This step may need to be repeated]\n",
    "\n",
    "With model in hand, we now need to evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the best metric for evaluating the selected model?\n",
    "\n",
    "There are many metrics we can use to evalute a model's performance, and we will cover a few of those now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before we evaluate the model, we need to split the data into a training and test set (described above). We can easily do this using [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from the [`model_selection`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) `scikit-learn` subpackage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, \n",
    "                                                    iris.target, \n",
    "                                                    test_size = 0.3, \n",
    "                                                    random_state = 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why random_state = 23? Because we are in Chicago, and Michael Jordan was on the Bulls, and Michael Jordan is the best basketball player ever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At this stage - we set the test set aside, and ignore it completely until we have fully optimized our model.\n",
    "\n",
    "Applying the model to these data before finalizing the model is SNOOPING - don't do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Terminology\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **True Positive** (TP)   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; + classified as +  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **False Positive** (FP)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - classified as +  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **True Negative** (TN)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - classified as -  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **False Negative** (FN)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; + classified as - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Terminology - Most metrics are defined by [TP, FP, TN, and FN](https://en.wikipedia.org/wiki/Sensitivity_and_specificity):\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **Accuracy**    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (TP + TN)/(TP + TN + FP + FN)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **True Positive Rate** (aka sensitivity, recall, etc)   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TP/(TP + FN)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **False Positive Rate**   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; FP/(TN + FP)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **True Negative Rate** (aka specificity)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TN/(TN + FP)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **Precision**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TP/(TP + FP)  \n",
    "\n",
    "and many, many more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another extremely useful tool is the [Receiver Operating Characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve. For $k$NN it is not possible to determine the ROC curve, as the ROC curve is determined by measuring the TPR vs. FPR as a function of varying classification decision thresholds. Models that produce probabilistic classifications can be used to create ROC curves.\n",
    "\n",
    "The ROC curve is extremely useful for setting decision thresholds in cases where a desired TPR or FPR is known a priori (e.g., when to trigger human review of credit card transactions in possible cases of fraud). When \"follow-up\" resources are limited setting these thresholds helps to optimize performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) is exceptionally useful for identifying classes that are being misclassified:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "  <td> </td><td> </td><td colspan=\"2\">True Class</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td> </td><td> </td> <td> + </td><td> - </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td> predicted </td><td> + </td> <td> TP </td><td> FN </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td> class </td><td> - </td> <td> FP </td><td> TN </td> \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we cannot touch the test set, how do we evaluate the model performance?\n",
    "\n",
    "[Cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics). In brief, we will further split the training set, use some of it to define the mapping between features and labels, and then evaluate the quality of that mapping using the sources that were withheld from training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are many flavors of CV, but $k$-fold CV is most common. In $k$-fold CV, the training set is split into $k$ partitions. Iteratively, each partion is withheld, the model is trained, and predictions are made on the withheld partition. With predictions for every source in hand, we can compare the predictions to the known labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cross validation is simple with `scikit-learn` using the `model_selection` subpackage. We will focus on [`cross_val_predict`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) which returns predictions for every source in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_preds = cross_val_predict(knn_clf, X_train, y_train, cv = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The super useful [`metrics`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) subpackage allows us to evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "print(\"kNN CV acc = {:.4f}\".format(accuracy_score(y_train, \n",
    "                                                  y_train_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also use `scikit-learn` to make a confusion matrix. \n",
    "\n",
    "A nice looking confusion matrix requires [more code than fits on a slide](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py). I'll hide that and instead just show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cmap = sns.cubehelix_palette(8, start=.5, rot=-.75, as_cmap=True)\n",
    "cm = confusion_matrix(y_train, y_train_preds)\n",
    "with sns.axes_style(\"white\"):\n",
    "    plot_confusion_matrix(cm, iris.target_names, \n",
    "                          normalize = True, \n",
    "                          cmap = cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Model Optimization\n",
    "\n",
    "[This step may need to be repeated]\n",
    "\n",
    "Previously, we set $k = 11$ for the $k$NN model, but we (rightly) asked, what is so special about 11?\n",
    "\n",
    "Now we should optimize the model tuning parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The tried and true method in this regard is brute force: an exhaustive grid search across all relevant tuning parameters.\n",
    "\n",
    "In cases with many parameters (or extremely large data sets) a [randomized parameter search may be more pragmatic](http://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unfortunately, there is no substitute for patience. \n",
    "\n",
    "It is virtually never the case that some objective function can be optimized to determine the optimal tuning parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using the `model_selection` package, we can use the [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) class to perform the exhaustive search.\n",
    "\n",
    "In this case, the relevant parameters are $k$ and $p$, the order of the [Minkowski distance](https://en.wikipedia.org/wiki/Minkowski_distance) ($p = 2$ is equivalent to Euclidean distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "opt_tune = GridSearchCV(knn_clf, {'n_neighbors': [1, 3, 5, 10, 30, 50], \n",
    "                                  'p': [1, 2, 3]}, cv = 10)\n",
    "opt_tune.fit(X_train, y_train)\n",
    "opt_k = opt_tune.best_params_['n_neighbors']\n",
    "opt_p = opt_tune.best_params_['p']\n",
    "print(\"Opt model has k = {:d} and p = {:d}\".format(opt_k, opt_p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Furthermore, it is useful to understand how the performance changes as a function of the parameters in the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "k_grid = np.unique(opt_tune.cv_results_['param_n_neighbors'])\n",
    "p_grid = np.unique(opt_tune.cv_results_['param_p'])\n",
    "K, P = np.meshgrid(k_grid, p_grid)\n",
    "\n",
    "score_grid = np.empty(np.shape(K))\n",
    "for params, acc in zip(opt_tune.cv_results_['params'], \n",
    "                       opt_tune.cv_results_['mean_test_score']):\n",
    "\n",
    "    this_source = np.where((K == params['n_neighbors']) & (P == params['p']))\n",
    "    score_grid[this_source] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with sns.axes_style('white'):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(score_grid, origin = 'lower_left', \n",
    "                   cmap = cmap)\n",
    "\n",
    "    thresh = 0.92\n",
    "    for i, j in itertools.product(range(score_grid.shape[0]), \n",
    "                                  range(score_grid.shape[1])):\n",
    "        ax.text(j, i, format(score_grid[i, j], '.4f'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"w\" if score_grid[i, j] > thresh else \"k\")\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(k_grid)))\n",
    "    ax.set_xticklabels(k_grid)\n",
    "    ax.set_yticks(np.arange(len(p_grid)))\n",
    "    ax.set_yticklabels(p_grid)\n",
    "    ax.set_xlabel('k', fontsize = 14)\n",
    "    ax.set_ylabel('p', fontsize = 14)\n",
    "    cb = plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this case we see that a range of different parameter choices provide the optimal results. Moving forward we will make predictions using models with $k = 10$ and $p = 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. Prediction\n",
    "\n",
    "The final step, now that we have fully specified and trained our model, is to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Apply the model to the test set $\\rightarrow$ estimate the generalization error (i.e. how does the model perform on new data?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The test-set generalization error typically overestimates the model performance. There are several reasons why this might be the case, but the easiest to understand is training set bias. \n",
    "\n",
    "Every source that has a label is labeled \"for a reason\" (typically because someone, somewhere decided to devote resources towards labeling). This selection process is rarely random, meaning the training set is biased relative to the population. These biases, even if small, will be propagated through the model, but not identified via the test set, which (in most cases) comes from the same super set as the training set. \n",
    "\n",
    "As always - **worry about the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pulling back the curtain on the test set is a point of no return. \n",
    "\n",
    "At this stage, it's possible (and encouraged if necessary) to go back and adjust the work in section 2 (feature engineering), 3 (model selection), 4 (model evaluation) and 5 (model optimization). \n",
    "\n",
    "Cycling through these procedures multiple times is typically needed prior to evaluating the model with the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We, however, will proceed with our simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "knn_clf_final = KNeighborsClassifier(n_neighbors=3, p = 2)\n",
    "knn_clf_final.fit(X_train, y_train)\n",
    "\n",
    "test_preds = knn_clf_final.predict(X_test)\n",
    "\n",
    "gen_error = 1 - accuracy_score(y_test, test_preds)\n",
    "\n",
    "print(\"The kNN test-set acc = {:.4f}\".format(1 - gen_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Interestingly, the test-set accuracy is $\\approx$ the CV accuracy.\n",
    "\n",
    "Thus, we should not expect this model to classify newly observed Iris flowers with an accuracy higher than 97%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Of course, the most interesting part of any ML model is applying it to unlabeled data.\n",
    "\n",
    "Unfortunately, we do not have unlabeled Iris data.\n",
    "\n",
    "The steps described above lay out the path to build an end-to-end, production ready machine learning model. Using this approach, it should be possible for you to construct and train models for prediction within your own disciplines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading List\n",
    "\n",
    "The standard for learning about Machine Learning is [*The Elements of Machine Learning*](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) by Hastie, Tibshirani, & Friedman.\n",
    "\n",
    "[`scikit-learn`](http://scikit-learn.org/stable/) is filled with examples and (brief) tutorials that cover a great deal of material beyond what was discussed today.\n",
    "\n",
    "Finally, [*Real World Machine Learning*](https://www.manning.com/books/real-world-machine-learning) by Brink, Richards, and Fetherolf is a great, approachable text for those beginning with machine learning. Much of this lecture was inspired by principles from this book. *Full disclosure* - this book was written by my friends, but if you can't shill for your friends, then who can you shill for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading List 2\n",
    "\n",
    "At present, the most exciting tools in machine learning are related to neural nets and \"deep learning.\" There are 2 dominant libraries for these methods:\n",
    "\n",
    "[`TensorFlow`](https://www.tensorflow.org)\n",
    "\n",
    "[`Theano`](https://github.com/Theano/Theano)\n",
    "\n",
    "Both of these libraries can be run in Python using [`Keras`](https://keras.io), a great place to start if you've never used these packages before.\n",
    "\n",
    "There's a LOT written about these libraries and different forms of deep learning, but if you're looking for a place to start [*Deep Learning in Python*](https://www.manning.com/books/deep-learning-with-python) by Francois Chollet (author of `Keras`) would be a good place to start.\n",
    "\n",
    "Additionally I'll note that some researchers have put together a package that in principle runs through all the steps I described above: [`auto-sklearn`](http://automl.github.io/auto-sklearn/stable/). While this package looks interesting and powerful, I would not recommend diving in here, without going through the above steps a few times. *Note - there is no substitution for domain knowledge*"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "livereveal": {
   "height": 768,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "solarized",
   "width": 1024
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
